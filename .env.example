# Server configuration
# The hostname for the server (default is "localhost")
HOST=localhost

# The port the server will listen on (default is "8081")
PORT=8081

# Engine type: "beam" or "ols"
# Set this to define which engine is used
# ENGINE=beam
ENGINE=ols

# Language model configuration
# Name of the language model (replace with your model name)
LLM_NAME=llama3:latest

# Set LLM_API manually if needed, but typically leave it as-is.
# For beam: "http://localhost:11434/api/generate"
# For ols: "http://localhost:8080"
# LLM_API=http://localhost:11434/api/generate
LLM_API=http://localhost:8080/v1/query